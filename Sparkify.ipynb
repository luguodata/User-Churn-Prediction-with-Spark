{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project \n",
    "\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "**Target**: Predict users at risk to churn either downgrade from premium to free tier or cancelling their services altogether.\n",
    "\n",
    "`1.` Load Dataset <br>\n",
    "`2.` Explore and Clean Data. <br>\n",
    "`3.` Feature Engineering. <br>\n",
    "`4.` Build Model. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import udf, sum, desc, lit, date_format, from_unixtime, isnan, when, count, col, substring\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path and read in\n",
    "data_path = \"mini_sparkify_event_data.json\"\n",
    "mini_data = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the table content\n",
    "mini_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows in this dataset\n",
    "mini_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the mini data as a SQL temporary view\n",
    "mini_data.createOrReplaceTempView(\"df_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "mini_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                   226|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count distinct user_ids\n",
    "spark.sql(\"select count(distinct(userId)) from df_mini\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artist', 58392),\n",
       " ('auth', 0),\n",
       " ('firstName', 8346),\n",
       " ('gender', 8346),\n",
       " ('itemInSession', 0),\n",
       " ('lastName', 8346),\n",
       " ('length', 58392),\n",
       " ('level', 0),\n",
       " ('location', 8346),\n",
       " ('method', 0),\n",
       " ('page', 0),\n",
       " ('registration', 8346),\n",
       " ('sessionId', 0),\n",
       " ('song', 58392),\n",
       " ('status', 0),\n",
       " ('ts', 0),\n",
       " ('userAgent', 8346),\n",
       " ('userId', 0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out null sums in each column \n",
    "[(c, spark.sql(\"select * from df_mini where {} is Null\".format(c)).count()) for c in mini_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artist', 0),\n",
       " ('auth', 0),\n",
       " ('firstName', 0),\n",
       " ('gender', 0),\n",
       " ('itemInSession', 0),\n",
       " ('lastName', 0),\n",
       " ('length', 0),\n",
       " ('level', 0),\n",
       " ('location', 0),\n",
       " ('method', 0),\n",
       " ('page', 0),\n",
       " ('registration', 0),\n",
       " ('sessionId', 0),\n",
       " ('song', 0),\n",
       " ('status', 0),\n",
       " ('ts', 0),\n",
       " ('userAgent', 0),\n",
       " ('userId', 8346)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out '' sums in each column \n",
    "[(c, spark.sql(\"select * from df_mini where {} = ''\".format(c)).count()) for c in mini_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    page|\n",
      "+--------+\n",
      "|NextSong|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look which pages are user in if \n",
    "spark.sql(\"select distinct page from df_mini where artist is not Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|song|\n",
      "+----+\n",
      "|null|\n",
      "|null|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look which pages are user in if \n",
    "spark.sql(\"select song from df_mini where userId = ''\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:** <br>\n",
    "\n",
    "    There are two types missing values in this dataset.\n",
    "    * Nulls: artist/song -- 58,392. Due to the log page is not 'NextSong'.\n",
    "    * ''s: userId -- 8,346. Due to user not log in. At this situation, user related information would be nulls such        as first name,last name,location etc.\n",
    "    \n",
    "    Next step is to keep logged in users only -- remove userId is '' records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out userId is null records\n",
    "mini_data_non0 = mini_data.filter(mini_data.userId != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_data_non0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data_non0.createOrReplaceTempView(\"df_mini_non0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artist', 50046),\n",
       " ('auth', 0),\n",
       " ('firstName', 0),\n",
       " ('gender', 0),\n",
       " ('itemInSession', 0),\n",
       " ('lastName', 0),\n",
       " ('length', 50046),\n",
       " ('level', 0),\n",
       " ('location', 0),\n",
       " ('method', 0),\n",
       " ('page', 0),\n",
       " ('registration', 0),\n",
       " ('sessionId', 0),\n",
       " ('song', 50046),\n",
       " ('status', 0),\n",
       " ('ts', 0),\n",
       " ('userAgent', 0),\n",
       " ('userId', 0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out null sums in each column \n",
    "[(c, spark.sql(\"select * from df_mini_non0 where {} is Null\".format(c)).count()) for c in mini_data_non0.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispaly_vara_cnt(vara):\n",
    "    \"\"\" Show variable's unique count, if unique values less than 10 print them out.\n",
    "    INPUT:\n",
    "    vara: -- (string), variable need to be counted.\n",
    "    OUTPUT:\n",
    "    total counts or groupby counts\n",
    "    \"\"\"\n",
    "    ttl_uniq_cnt = mini_data_non0.select([vara]).dropDuplicates().count()\n",
    "    if ttl_uniq_cnt >10:\n",
    "        print(\"In this dataset, there are total {} unique {}\".format(ttl_uniq_cnt, vara))\n",
    "        \n",
    "    elif vara in ['gender','firstName','lastnAME','location']:\n",
    "        spark.sql(\"select {}, count(*) as unique_cnt from \\\n",
    "          (select distinct userId, {} from df_mini_non0) \\\n",
    "          group by {}\".format(vara, vara, vara)).show()\n",
    "        \n",
    "    else:\n",
    "        spark.sql(\"select {}, count(level) as unique_cnt from df_mini_non0\\\n",
    "           group by {} order by unique_cnt desc\".format(vara, vara)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 225 unique userId\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|gender|unique_cnt|\n",
      "+------+----------+\n",
      "|     F|       104|\n",
      "|     M|       121|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 17656 unique artist\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 1311 unique itemInSession\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('itemInSession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 14866 unique length\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|level|unique_cnt|\n",
      "+-----+----------+\n",
      "| paid|    222433|\n",
      "| free|     55721|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 114 unique location\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|method|unique_cnt|\n",
      "+------+----------+\n",
      "|   PUT|    257818|\n",
      "|   GET|     20336|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 19 unique page\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 225 unique registration\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('registration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 58481 unique song\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('song')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|status|unique_cnt|\n",
      "+------+----------+\n",
      "|   200|    254718|\n",
      "|   307|     23184|\n",
      "|   404|       252|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 56 unique userAgent\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('userAgent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporarily to keep gender, level, location, page, method, registration, status, userAgent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Churn Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Churn\n",
    "# To look how many unique users submit cancellation\n",
    "mini_data_non0.select(['userId']).where(mini_data_non0.page.isin(['Cancellation Confirmation']))\\\n",
    "            .dropDuplicates().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # To look how many unique users submit cancellation or Downgrade\n",
    "mini_data_non0.select(['userId']).where(mini_data_non0.page.isin(['Cancellation Confirmation','Downgrade']))\\\n",
    "                .dropDuplicates().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total users\n",
    "mini_data_non0.select(['userId']).dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "1. There are total 225 unique users in this dataset.\n",
    "2. Out of these 225 unique users, 52 submitted cancellations and 171 submitted either cancellations of downgrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column to label cancellation or not\n",
    "# define churn function\n",
    "cancellation_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "# apply churn function\n",
    "mini_data_non0 = mini_data_non0.withColumn(\"churn\", cancellation_event(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill churn user na as 0s\n",
    "mini_data_non0 = mini_data_non0.na.fill({'churn_user': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hour from ts\n",
    "ts_to_hour = udf(lambda x: datetime.fromtimestamp(x / 1000.0).hour)\n",
    "mini_data_non0 = mini_data_non0.withColumn(\"hour\",ts_to_hour(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_to_day = udf(lambda x: datetime.fromtimestamp(x / 1000.0).day)\n",
    "mini_data_non0 = mini_data_non0.withColumn(\"day\",ts_to_day(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data_non0 = mini_data_non0.withColumn(\"date\",from_unixtime(mini_data_non0.ts/1000).cast(DateType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To explore which columns kept between churn/not churn groups**\n",
    "\n",
    "**Temporarily to keep gender, level, location, page, method, registration, status, userAgent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data_non0.createOrReplaceTempView(\"df_mini_non0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|churn_user|\n",
      "+----------+----------+\n",
      "|2018-10-01|         1|\n",
      "|2018-10-02|         1|\n",
      "|2018-10-04|         2|\n",
      "|2018-10-05|         1|\n",
      "|2018-10-07|         2|\n",
      "|2018-10-08|         1|\n",
      "|2018-10-11|         1|\n",
      "|2018-10-12|         2|\n",
      "|2018-10-13|         2|\n",
      "|2018-10-15|         2|\n",
      "|2018-10-16|         2|\n",
      "|2018-10-17|         2|\n",
      "|2018-10-19|         1|\n",
      "|2018-10-20|         3|\n",
      "|2018-10-22|         2|\n",
      "|2018-10-23|         2|\n",
      "|2018-10-24|         1|\n",
      "|2018-10-26|         1|\n",
      "|2018-10-30|         1|\n",
      "|2018-11-01|         2|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# churn users' churn date distirbution\n",
    "spark.sql(\"select date, count(date) as churn_user \\\n",
    "           from df_mini_non0 where churn = 1 \\\n",
    "           group by date order by date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2845"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Count how many rows of unique user with dates -- Considering put churn as the rolling weekly base\n",
    "spark.sql(\"select date, userId from df_mini_non0 \\\n",
    "           where date <= '2018-11-25' \\\n",
    "           group by date, userId\").count()\n",
    "\n",
    "# Will generate 2,845 records for predicting next week churn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts:** <br>\n",
    "If we observe monthly churn, then I will select 2018-11-01 as the benchmark date, 2018-10-01 to 2018-10-31 as the observational period and customers active at 2018-10-31 as base customer counts. Customers who churned between 2018-11-01 and 2018-11-30 as target churned customers. All customers who churned before 2018-11-01 will be removed from the analysis. In other words, only keep customers who were active at 2018-10-31. \n",
    "* Baseline: (214 - (52-22)) = 184.\n",
    "* Churned in the folowing month: 22.\n",
    "* Monthly Churn Rate: 22/184 = 12%\n",
    "\n",
    "Also, checked if using rolling weekly base for predicting next week churn, that will generate 2,845 rows insteading of 195 rows.\n",
    "\n",
    "**Next Step**:<br>\n",
    "1) Filtered user logs who churned before 2018-11-01. <br>\n",
    "2) Aggregate user activities during observational period (2018-10-01 to 2018-10-31) <br>\n",
    "3) Compare the difference between churn/stay groups. <br>\n",
    "4) If time allowed, try rolling weekly method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Get obervational period data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data_obv = spark.sql(\"select * from df_mini_non0 \\\n",
    "                          where date >= '2018-10-01' and date < '2018-11-01'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Remove users churned between 2018-10-01 and 2018-11-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get unique churn users\n",
    "churn_user_obv = mini_data_obv.select(['userId']).where(mini_data_obv.churn == 1).dropDuplicates()\n",
    "# assign new churn user label to churn users\n",
    "churn_user_obv = churn_user_obv.withColumn(\"churn_user_obv\", lit(1))\n",
    "# join churn user back to original table and got an label\n",
    "mini_data_obv = mini_data_obv.join(churn_user_obv, \"userId\", how = 'outer')\n",
    "# remove users records who churn between 2028-10-01 and 2018-11-01\n",
    "mini_data_obv = mini_data_obv.where(col('churn_user_obv').isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.`Get unique userIds who churned during target period 2018-11-01 and 2018-12-01, then join back with the observational period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Churn period data\n",
    "mini_data_target = spark.sql(\"select * from df_mini_non0 \\\n",
    "                          where date >= '2018-11-01' and date < '2018-12-01'\")\n",
    "\n",
    "# unique users churned during the target period\n",
    "churn_user_target = mini_data_target.select(['userId']).where(mini_data_target.churn == 1).dropDuplicates()\n",
    "\n",
    "# assign new churn user label to churn users\n",
    "churn_user_target = churn_user_target.withColumn(\"churn_user_target\", lit(1))\n",
    "\n",
    "# join churn user back to observal table and got an label\n",
    "mini_data_obv = mini_data_obv.join(churn_user_target, \"userId\", how = 'outer')\n",
    "\n",
    "# fill na with 0s in the observational dataset\n",
    "mini_data_obv = mini_data_obv.fillna(0, subset = ['churn_user_target'])\n",
    "\n",
    "# Drop some columns\n",
    "mini_data_obv = mini_data_obv.drop('churn').drop('churn_user_obv').drop('month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data_obv.createOrReplaceTempView(\"df_mini_non0_obv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|churn_user_target|   cnt|\n",
      "+-----------------+------+\n",
      "|                0|108206|\n",
      "|                1| 20168|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# churn/not churn counts\n",
    "spark.sql(\"select churn_user_target, count(*) as cnt from df_mini_non0_obv\\\n",
    "           group by churn_user_target order by churn_user_target\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|churn_user_target|cnt|\n",
      "+-----------------+---+\n",
      "|                0|162|\n",
      "|                1| 22|\n",
      "+-----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unique churn/nonchurn user counts\n",
    "spark.sql(\"select churn_user_target, count(*) as cnt from \\\n",
    "          (select distinct userId,  churn_user_target from df_mini_non0_obv)\\\n",
    "          group by churn_user_target order by churn_user_target\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Transform dataframe into user - feature format. Total would be 184 rows and many columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gender\n",
    "df_gender = spark.sql(\"select distinct userId, gender, churn_user_target from df_mini_non0_obv\")\n",
    "df_gender.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location\n",
    "df_location = spark.sql(\"select distinct userId, location, churn_user_target from df_mini_non0_obv\")\n",
    "# extract State -- 37\n",
    "substr_state = udf(lambda x: x[-2:], StringType())\n",
    "df_location = df_location.withColumn(\"state\", substr_state(\"location\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|opr_system|\n",
      "+----------+\n",
      "|  iPad; CP|\n",
      "|  Windows |\n",
      "|  compatib|\n",
      "|  Macintos|\n",
      "|  iPhone; |\n",
      "|  X11; Ubu|\n",
      "|  X11; Lin|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# userAgent\n",
    "df_useAgen = spark.sql(\"select distinct userId, userAgent, churn_user_target from df_mini_non0_obv\")\n",
    "remove_qot = udf(lambda x: x.replace(u'\"',''))\n",
    "substr_agent = udf(lambda x: x[13:21], StringType())\n",
    "df_useAgen = df_useAgen.withColumn(\"opr_system\", substr_agent(remove_qot(\"userAgent\")))\n",
    "df_useAgen.select(['opr_system']).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|         ttl_song|\n",
      "+-------+-----------------+\n",
      "|  count|              183|\n",
      "|   mean|572.1584699453551|\n",
      "| stddev| 649.292211448894|\n",
      "|    min|                1|\n",
      "|    max|             5127|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total songs played during the month\n",
    "df_song = spark.sql(\"select userId, count(song) as ttl_song\\\n",
    "                     from df_mini_non0_obv \\\n",
    "                     where song is not Null \\\n",
    "                     group by userId\")\n",
    "\n",
    "df_song.describe('ttl_song').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+\n",
      "|userId|About|Add Friend|Add to Playlist|Downgrade|Error|Help|Home|Logout|NextSong|Roll Advert|Save Settings|Settings|Submit Downgrade|Submit Upgrade|Thumbs Down|Thumbs Up|Upgrade|\n",
      "+------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+\n",
      "|200002|    2|         4|              6|        3| null|   1|  14|     3|     267|          7|         null|       3|            null|             1|          6|       15|      2|\n",
      "|100010| null|         3|              2|     null| null|   1|   6|     2|     120|         22|         null|    null|            null|          null|          1|        6|      1|\n",
      "+------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pages\n",
    "# reshap page count -- userId as row and page as column, count as value\n",
    "df_page = mini_data_obv.groupby('userId').pivot('page').count()\n",
    "df_page.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- churn_user_target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mini_data_obv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|   avg_song_length|\n",
      "+-------+------------------+\n",
      "|  count|               183|\n",
      "|   mean|248.80327868852459|\n",
      "| stddev| 7.295534549021886|\n",
      "|    min|             222.0|\n",
      "|    max|             280.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "df_song_length = spark.sql(\"select userId, round(avg(length)) as avg_song_length\\\n",
    "                        from df_mini_non0_obv \\\n",
    "                        where song is not Null \\\n",
    "                        group by userId\")\n",
    "\n",
    "df_song_length.describe('avg_song_length').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------------+\n",
      "|userId|level|churn_user_target|\n",
      "+------+-----+-----------------+\n",
      "|   146| free|                0|\n",
      "|    38| paid|                0|\n",
      "|    88| free|                0|\n",
      "|   126| paid|                0|\n",
      "|   132| paid|                0|\n",
      "|   104| paid|                0|\n",
      "|300021| paid|                0|\n",
      "|    82| free|                0|\n",
      "|300018| free|                0|\n",
      "|    10| paid|                0|\n",
      "|   120| paid|                0|\n",
      "|300011| paid|                0|\n",
      "|    81| paid|                0|\n",
      "|300019| free|                0|\n",
      "|    40| paid|                0|\n",
      "|    70| free|                1|\n",
      "|   127| paid|                0|\n",
      "|     8| free|                0|\n",
      "|200017| paid|                1|\n",
      "|    28| free|                1|\n",
      "+------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# level -- window function to mark the changes free as 0, paid as 1 -- how many days free, how many days paid\n",
    "df_level = spark.sql(\"select distinct userId, level, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_level.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|userId|cnt|\n",
      "+------+---+\n",
      "|200002|  2|\n",
      "|100010|  1|\n",
      "|     7|  1|\n",
      "|   124|  1|\n",
      "|    54|  2|\n",
      "|    15|  1|\n",
      "|   132|  2|\n",
      "|100014|  1|\n",
      "|    11|  2|\n",
      "|   138|  2|\n",
      "|300017|  1|\n",
      "|    29|  2|\n",
      "|    69|  2|\n",
      "|100021|  1|\n",
      "|    42|  2|\n",
      "|   112|  1|\n",
      "|200010|  1|\n",
      "|    64|  1|\n",
      "|    30|  2|\n",
      "|   113|  2|\n",
      "+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select userId, count(*) as cnt from \\\n",
    "         (select distinct userId, level, churn_user_target from df_mini_non0_obv)\\\n",
    "           group by userId sort by cnt desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method -- TBD\n",
    "df_method = spark.sql(\"select distinct userId, method, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_method.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct method from df_mini_non0_obv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------------+\n",
      "|userId| registration|churn_user_target|\n",
      "+------+-------------+-----------------+\n",
      "|    45|1536398117000|                0|\n",
      "|   118|1537893493000|                0|\n",
      "|300016|1534622171000|                0|\n",
      "|    11|1532554781000|                0|\n",
      "|    13|1533192032000|                0|\n",
      "|300012|1530306321000|                0|\n",
      "|300019|1536158069000|                0|\n",
      "|    36|1533908361000|                0|\n",
      "|    90|1533995214000|                0|\n",
      "|    57|1535062159000|                0|\n",
      "|300011|1538336771000|                0|\n",
      "|    76|1538065863000|                0|\n",
      "|    92|1536403972000|                0|\n",
      "|300022|1534461078000|                0|\n",
      "|    60|1537014411000|                0|\n",
      "|     8|1533650280000|                0|\n",
      "|    88|1536663902000|                0|\n",
      "|    62|1531804365000|                0|\n",
      "|200008|1533670697000|                0|\n",
      "|200006|1536963671000|                0|\n",
      "+------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# registration -- drop since each user has an distinct regi number\n",
    "df_regi = spark.sql(\"select distinct userId, registration, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_regi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1619"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sessionId -- drop since too much level\n",
    "df_session = spark.sql(\"select distinct userId, sessionId, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_session.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# status -- need to figure out how the status changes for each user\n",
    "df_status = spark.sql(\"select distinct userId, status, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_status.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct status from df_mini_non0_obv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Step** <br>\n",
    "\n",
    "* compile all separated functions together. sql + substr + pivot table\n",
    "* Distribution on two classes\n",
    "* Fit ML model\n",
    "* Move to AWS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2247"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select userId, page, count(*) as page_cnt from df_mini_non0_obv \\\n",
    "           group by userId, page\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_data_obv.groupby('userId').pivot('page').count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gentder counts\n",
    "spark.sql(\"select gender, churn_user, count(*) as cnt from \\\n",
    "          (select distinct userId, gender, churn_user from df_mini_non0)\\\n",
    "          group by gender, churn_user order by churn_user, gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level counts\n",
    "spark.sql(\"select level, churn_user, count(*) as cnt \\\n",
    "           from df_mini_non0 group by level, churn_user order by churn_user, level\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page counts\n",
    "spark.sql(\"select page, churn_user, count(*) as cnt \\\n",
    "           from df_mini_non0 group by page, churn_user order by churn_user, page\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# song counts\n",
    "spark.sql(\"select churn_user, count(song) as song_cnt \\\n",
    "           from df_mini_non0 group by churn_user order by churn_user\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "spark.sql(\"select method, churn_user, count(*) as cnt \\\n",
    "           from df_mini_non0 group by method, churn_user order by churn_user, method\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mini_data_non0.select(\"hour\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played.\n",
    "\n",
    "rows: unique user\n",
    "Columns:\n",
    "1. number of songs played\n",
    "2. actions taken and # of these actions\n",
    "3. gender\n",
    "4. level\n",
    "5. extract user agent\n",
    "6. udf hour, day\n",
    "7. location\n",
    "8. method\n",
    "\n",
    "\n",
    "Qs:\n",
    "1. How to define time unit\n",
    "2. How to agg during each time unit\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender table\n",
    "user_gender = spark.sql(\"SELECT distinct userId, gender from mini_FE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location\n",
    "user_location = spark.sql(\"SELECT distinct userId, location from mini_FE\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
