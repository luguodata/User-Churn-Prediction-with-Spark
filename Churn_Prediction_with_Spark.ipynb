{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project \n",
    "\n",
    "\n",
    "**Project Overview:** \n",
    "This project is about building a churn prediction model on Sparkify, a Digital music service, like Spotify or Pandora. Spark will be used to manipulate the user log information ,do data exploratory and build machine learning models for predict monthly churn.\n",
    "\n",
    "A subset of full data will be used for this analysis. \n",
    "\n",
    "**Project Purpose:** \n",
    "Churn prevention is a hot and challenging problem in almost every product and service company. If the risk of users disconnecting service could be predicted, then company could take actions to save customers before they leaving. In this sepecific case for Sparkify, what I do is to predict users at risk to cancel their services or downgrading from premium to free tier altogether. More sepecifically, I will use the monthly user-product interations along with user demographic information to predict next month churn. If we could get good model performance and make it into production, company would benefit a lot.\n",
    "\n",
    "**Project Evaluation**\n",
    "Since customer churn or not is a binary outcome, the classification models will be built on the processed dataset. \n",
    "Churn or stay group is imblanced, F1 score, AUC score will be as the evalution metrics inteading of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "**Target**: Predict users at risk to churn either downgrade from premium to free tier or cancelling their services altogether.\n",
    "\n",
    "`1.` Load and Clean Dataset <br>\n",
    "`2.` Feature Engineering. <br>\n",
    "`3.` Build Data Processing and ML Pipeline. <br>\n",
    "`4.` Model Training and Prediction. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import necessary libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a SparkSession for using spark and name it \"Sparkify\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "Load and clean the dataset, remove invalid records such as no userId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set data path and read in\n",
    "data_path = \"mini_sparkify_event_data.json\"\n",
    "mini_data = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+------+-------------+--------+---------+-----+---------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+\n",
      "|        artist|     auth|firstName|gender|itemInSession|lastName|   length|level|       location|method|    page| registration|sessionId|     song|status|           ts|           userAgent|userId|\n",
      "+--------------+---------+---------+------+-------------+--------+---------+-----+---------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+\n",
      "|Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "+--------------+---------+---------+------+-------------+--------+---------+-----+---------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the table content\n",
    "mini_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows in this dataset\n",
    "mini_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "mini_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the mini data as a SQL temporary view\n",
    "mini_data.createOrReplaceTempView(\"df_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                   226|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count distinct user_ids\n",
    "spark.sql(\"select count(distinct(userId)) from df_mini\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artist', 58392),\n",
       " ('auth', 0),\n",
       " ('firstName', 8346),\n",
       " ('gender', 8346),\n",
       " ('itemInSession', 0),\n",
       " ('lastName', 8346),\n",
       " ('length', 58392),\n",
       " ('level', 0),\n",
       " ('location', 8346),\n",
       " ('method', 0),\n",
       " ('page', 0),\n",
       " ('registration', 8346),\n",
       " ('sessionId', 0),\n",
       " ('song', 58392),\n",
       " ('status', 0),\n",
       " ('ts', 0),\n",
       " ('userAgent', 8346),\n",
       " ('userId', 0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out null sums in each column \n",
    "[(c, spark.sql(\"select * from df_mini where {} is Null\".format(c)).count()) for c in mini_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artist', 0),\n",
       " ('auth', 0),\n",
       " ('firstName', 0),\n",
       " ('gender', 0),\n",
       " ('itemInSession', 0),\n",
       " ('lastName', 0),\n",
       " ('length', 0),\n",
       " ('level', 0),\n",
       " ('location', 0),\n",
       " ('method', 0),\n",
       " ('page', 0),\n",
       " ('registration', 0),\n",
       " ('sessionId', 0),\n",
       " ('song', 0),\n",
       " ('status', 0),\n",
       " ('ts', 0),\n",
       " ('userAgent', 0),\n",
       " ('userId', 8346)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out '' sums in each column \n",
    "[(c, spark.sql(\"select * from df_mini where {} = ''\".format(c)).count()) for c in mini_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    page|\n",
      "+--------+\n",
      "|NextSong|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look which pages are user in if \n",
    "spark.sql(\"select distinct page from df_mini where artist is not Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|song|\n",
      "+----+\n",
      "|null|\n",
      "|null|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look which pages are user in if \n",
    "spark.sql(\"select song from df_mini where userId = ''\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:** <br>\n",
    "\n",
    "    There are two types missing values in this dataset.\n",
    "    * Nulls: artist/song -- 58,392. Due to the log page is not 'NextSong'.\n",
    "    * ''s: userId -- 8,346. Due to user not log in. At this situation, user related information would be nulls such        as first name,last name,location etc.\n",
    "    \n",
    "    Next step is to keep logged in users only -- remove userId is '' records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out userId is null records\n",
    "mini_data_non0 = mini_data.filter(mini_data.userId != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_data_non0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data_non0.createOrReplaceTempView(\"df_mini_non0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artist', 50046),\n",
       " ('auth', 0),\n",
       " ('firstName', 0),\n",
       " ('gender', 0),\n",
       " ('itemInSession', 0),\n",
       " ('lastName', 0),\n",
       " ('length', 50046),\n",
       " ('level', 0),\n",
       " ('location', 0),\n",
       " ('method', 0),\n",
       " ('page', 0),\n",
       " ('registration', 0),\n",
       " ('sessionId', 0),\n",
       " ('song', 50046),\n",
       " ('status', 0),\n",
       " ('ts', 0),\n",
       " ('userAgent', 0),\n",
       " ('userId', 0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out null sums in each column \n",
    "[(c, spark.sql(\"select * from df_mini_non0 where {} is Null\".format(c)).count()) for c in mini_data_non0.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispaly_vara_cnt(vara):\n",
    "    \"\"\" Show variable's unique count, if unique values less than 10 print them out.\n",
    "    INPUT:\n",
    "    vara: -- (string), variable need to be counted.\n",
    "    OUTPUT:\n",
    "    total counts or groupby counts\n",
    "    \"\"\"\n",
    "    ttl_uniq_cnt = mini_data_non0.select([vara]).dropDuplicates().count()\n",
    "    if ttl_uniq_cnt >10:\n",
    "        print(\"In this dataset, there are total {} unique {}\".format(ttl_uniq_cnt, vara))\n",
    "        \n",
    "    elif vara in ['gender','firstName','lastnAME','location']:\n",
    "        spark.sql(\"select {}, count(*) as unique_cnt from \\\n",
    "          (select distinct userId, {} from df_mini_non0) \\\n",
    "          group by {}\".format(vara, vara, vara)).show()\n",
    "        \n",
    "    else:\n",
    "        spark.sql(\"select {}, count(level) as unique_cnt from df_mini_non0\\\n",
    "           group by {} order by unique_cnt desc\".format(vara, vara)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 225 unique userId\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|gender|unique_cnt|\n",
      "+------+----------+\n",
      "|     F|       104|\n",
      "|     M|       121|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 17656 unique artist\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 1311 unique itemInSession\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('itemInSession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|level|unique_cnt|\n",
      "+-----+----------+\n",
      "| paid|    222433|\n",
      "| free|     55721|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 114 unique location\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|method|unique_cnt|\n",
      "+------+----------+\n",
      "|   PUT|    257818|\n",
      "|   GET|     20336|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 19 unique page\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 225 unique registration\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('registration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 58481 unique song\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('song')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|status|unique_cnt|\n",
      "+------+----------+\n",
      "|   200|    254718|\n",
      "|   307|     23184|\n",
      "|   404|       252|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, there are total 56 unique userAgent\n"
     ]
    }
   ],
   "source": [
    "dispaly_vara_cnt('userAgent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "\n",
    "\n",
    "### Define Churn\n",
    "For a music service company, we could define churn as cancel service and even more downgrade service could be counted. I will create a column `Churn` by using either the `Cancellation Confirmation` events alone or with `Downgrade` events together. Will decide after exploring the event counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Churn\n",
    "# To look how many unique users submit cancellation\n",
    "mini_data_non0.select(['userId']).where(mini_data_non0.page.isin(['Cancellation Confirmation']))\\\n",
    "            .dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look how many unique users submit cancellation or Downgrade\n",
    "mini_data_non0.select(['userId']).where(mini_data_non0.page.isin(['Cancellation Confirmation','Downgrade']))\\\n",
    "                .dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total users\n",
    "mini_data_non0.select(['userId']).dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "1. There are total 225 unique users in this dataset.\n",
    "2. Out of these 225 unique users, 52 submitted cancellations and 171 submitted either cancellations or downgrades.\n",
    "\n",
    "Decide to use the `Cancellation Confirmation` events as churn label since the proportion of `Downgrade` users is too large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column to label cancellation or not\n",
    "# define churn function\n",
    "cancellation_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "# apply churn function\n",
    "mini_data_non0 = mini_data_non0.withColumn(\"churn\", cancellation_event(\"page\"))\n",
    "# fill churn user na as 0s\n",
    "mini_data_non0 = mini_data_non0.na.fill({'churn': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some time related cols from ts\n",
    "ts_to_hour = udf(lambda x: datetime.fromtimestamp(x / 1000.0).hour)\n",
    "mini_data_non0 = mini_data_non0.withColumn(\"hour\",ts_to_hour(\"ts\"))\n",
    "ts_to_day = udf(lambda x: datetime.fromtimestamp(x / 1000.0).day)\n",
    "mini_data_non0 = mini_data_non0.withColumn(\"day\",ts_to_day(\"ts\"))\n",
    "mini_data_non0 = mini_data_non0.withColumn(\"date\",from_unixtime(mini_data_non0.ts/1000).cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|churn_user|\n",
      "+----------+----------+\n",
      "|2018-10-01|         1|\n",
      "|2018-10-02|         1|\n",
      "|2018-10-04|         2|\n",
      "|2018-10-05|         1|\n",
      "|2018-10-07|         2|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set a temp sql view\n",
    "mini_data_non0.createOrReplaceTempView(\"df_mini_non0\")\n",
    "\n",
    "# churn users' churn date distirbution\n",
    "spark.sql(\"select date, count(date) as churn_user \\\n",
    "           from df_mini_non0 where churn = 1 \\\n",
    "           group by date order by date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                    22|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To see how many users churn between 2018-11-01 to 2018-11-31 \n",
    "spark.sql(\"select count(distinct(userId)) from df_mini_non0 \\\n",
    "                          where date >= '2018-11-01' and date < '2018-12-01' \\\n",
    "                          and churn = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts:** <br>\n",
    "\n",
    "In order to take actions before user churn, we need set-up an observation period and to predict user churn scenario in the upcoming target period. In real business world, month is common period for track the performance metric. Thus, in this project I'll use user monthly activities to predict next month churn. \n",
    "\n",
    "I will select 2018-11-01 as the benchmark date, 2018-10-01 to 2018-10-31 as the observational period and customers active at 2018-10-31 as base customer counts. Customers who churned between 2018-11-01 and 2018-11-30 as target churned customers. All customers who churned before 2018-11-01 will be removed from the analysis. In other words, only keep customers who were active at 2018-10-31. \n",
    "\n",
    "Unique customers:\n",
    "\n",
    "* Baseline: (214 - (52-22)) = 184.\n",
    "* Churned in the folowing month: 22.\n",
    "* Monthly Churn Rate: 22/184 = 12%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Get obervational period data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data_non0.createOrReplaceTempView(\"df_mini_non0\")\n",
    "\n",
    "mini_data_obv = spark.sql(\"select * from df_mini_non0 \\\n",
    "                          where date >= '2018-10-01' and date < '2018-11-01'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Remove users churned between 2018-10-01 and 2018-11-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get unique churn users\n",
    "churn_user_obv = mini_data_obv.select(['userId']).where(mini_data_obv.churn == 1).dropDuplicates()\n",
    "# assign new churn user label to churn users\n",
    "churn_user_obv = churn_user_obv.withColumn(\"churn_user_obv\", lit(1))\n",
    "# join churn user back to original table and got an label\n",
    "mini_data_obv = mini_data_obv.join(churn_user_obv, \"userId\", how = 'outer')\n",
    "# remove users records who churn between 2018-10-01 and 2018-11-01\n",
    "mini_data_obv = mini_data_obv.where(col('churn_user_obv').isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.`Get unique userIds who churned during target period 2018-11-01 and 2018-12-01, then join back with the observational period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Churn period data\n",
    "mini_data_target = spark.sql(\"select * from df_mini_non0 \\\n",
    "                          where date >= '2018-11-01' and date < '2018-12-01'\")\n",
    "\n",
    "# unique users churned during the target period\n",
    "churn_user_target = mini_data_target.select(['userId']).where(mini_data_target.churn == 1).dropDuplicates()\n",
    "\n",
    "# assign new churn user label to churn users\n",
    "churn_user_target = churn_user_target.withColumn(\"churn_user_target\", lit(1))\n",
    "\n",
    "# join churn user back to observal table and got an label\n",
    "mini_data_obv = mini_data_obv.join(churn_user_target, \"userId\", how = 'outer')\n",
    "\n",
    "# fill na with 0s in the observational dataset\n",
    "mini_data_obv = mini_data_obv.fillna(0, subset = ['churn_user_target'])\n",
    "\n",
    "# Drop some columns\n",
    "mini_data_obv = mini_data_obv.drop('churn').drop('churn_user_obv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|churn_user_target|   cnt|\n",
      "+-----------------+------+\n",
      "|                0|108206|\n",
      "|                1| 20168|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set temp sql view\n",
    "mini_data_obv.createOrReplaceTempView(\"df_mini_non0_obv\")\n",
    "\n",
    "# churn/not churn counts\n",
    "spark.sql(\"select churn_user_target, count(*) as cnt from df_mini_non0_obv\\\n",
    "           group by churn_user_target order by churn_user_target\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|churn_user_target|cnt|\n",
      "+-----------------+---+\n",
      "|                0|162|\n",
      "|                1| 22|\n",
      "+-----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unique churn/nonchurn user counts\n",
    "spark.sql(\"select churn_user_target, count(*) as cnt from \\\n",
    "          (select distinct userId,  churn_user_target from df_mini_non0_obv)\\\n",
    "          group by churn_user_target order by churn_user_target\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation and Explotary\n",
    "\n",
    "Transform raw log data into user - feature format dataframe. For example, for certain time period keep distinct values for each customer -- gender, aggregate (sum or average) -- total song played, pages visited etc, paid/free status change etc.\n",
    "\n",
    "Then perform some data exploratary between churn/stay group users.\n",
    "\n",
    "`4.` Transform dataframe into user - feature format. Total would be 184 rows and many columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gender\n",
    "df_gender = spark.sql(\"select distinct userId, gender, churn_user_target from df_mini_non0_obv\")\n",
    "df_gender.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|         ttl_song|\n",
      "+-------+-----------------+\n",
      "|  count|              183|\n",
      "|   mean|572.1584699453551|\n",
      "| stddev| 649.292211448894|\n",
      "|    min|                1|\n",
      "|    max|             5127|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total songs played during the month\n",
    "df_song = spark.sql(\"select userId, count(song) as ttl_song\\\n",
    "                     from df_mini_non0_obv \\\n",
    "                     where song is not Null \\\n",
    "                     group by userId\")\n",
    "\n",
    "df_song.describe('ttl_song').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|opr_system|\n",
      "+----------+\n",
      "|  iPad; CP|\n",
      "|  Windows |\n",
      "|  compatib|\n",
      "|  Macintos|\n",
      "|  iPhone; |\n",
      "|  X11; Ubu|\n",
      "|  X11; Lin|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# userAgent\n",
    "df_useAgen = spark.sql(\"select distinct userId, userAgent, churn_user_target from df_mini_non0_obv\")\n",
    "remove_qot = udf(lambda x: x.replace(u'\"',''))\n",
    "substr_agent = udf(lambda x: x[13:21], StringType())\n",
    "df_useAgen = df_useAgen.withColumn(\"opr_system\", substr_agent(remove_qot(\"userAgent\")))\n",
    "df_useAgen.select(['opr_system']).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# location\n",
    "df_location = spark.sql(\"select distinct userId, location, churn_user_target from df_mini_non0_obv\")\n",
    "# extract State -- 37\n",
    "substr_state = udf(lambda x: x[-2:], StringType())\n",
    "df_location = df_location.withColumn(\"state\", substr_state(\"location\"))\n",
    "df_location.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+\n",
      "|userId|About|Add Friend|Add to Playlist|Downgrade|Error|Help|Home|Logout|NextSong|Roll Advert|Save Settings|Settings|Submit Downgrade|Submit Upgrade|Thumbs Down|Thumbs Up|Upgrade|\n",
      "+------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+\n",
      "|200002|    2|         4|              6|        3|    0|   1|  14|     3|     267|          7|            0|       3|               0|             1|          6|       15|      2|\n",
      "|100010|    0|         3|              2|        0|    0|   1|   6|     2|     120|         22|            0|       0|               0|             0|          1|        6|      1|\n",
      "+------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pages\n",
    "# Use pivot table to transform them into user-page format and value as how many times user visited.\n",
    "# reshap page count -- userId as row and page as column, count as value\n",
    "df_page = mini_data_obv.groupby('userId').pivot('page').count()\n",
    "# fillna as 0s\n",
    "df_page = df_page.fillna(0)\n",
    "df_page.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_page.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- About: long (nullable = true)\n",
      " |-- Add Friend: long (nullable = true)\n",
      " |-- Add to Playlist: long (nullable = true)\n",
      " |-- Downgrade: long (nullable = true)\n",
      " |-- Error: long (nullable = true)\n",
      " |-- Help: long (nullable = true)\n",
      " |-- Home: long (nullable = true)\n",
      " |-- Logout: long (nullable = true)\n",
      " |-- NextSong: long (nullable = true)\n",
      " |-- Roll Advert: long (nullable = true)\n",
      " |-- Save Settings: long (nullable = true)\n",
      " |-- Settings: long (nullable = true)\n",
      " |-- Submit Downgrade: long (nullable = true)\n",
      " |-- Submit Upgrade: long (nullable = true)\n",
      " |-- Thumbs Down: long (nullable = true)\n",
      " |-- Thumbs Up: long (nullable = true)\n",
      " |-- Upgrade: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look page cols\n",
    "df_page.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|   avg_song_length|\n",
      "+-------+------------------+\n",
      "|  count|               183|\n",
      "|   mean|248.80327868852459|\n",
      "| stddev| 7.295534549021886|\n",
      "|    min|             222.0|\n",
      "|    max|             280.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# avg song length\n",
    "df_song_length = spark.sql(\"select userId, round(avg(length)) as avg_song_length\\\n",
    "                        from df_mini_non0_obv \\\n",
    "                        where song is not Null \\\n",
    "                        group by userId\")\n",
    "\n",
    "df_song_length.describe('avg_song_length').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts:**\n",
    "\n",
    "For \"level\", users might be paid/free level for the month, they could change from paid to free, free to paid in this month, maybe many times. So I created variables to track how many times each user from paid to free, from free to paid, and indicator of they staying paid/free for the whole month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect level change\n",
    "# reference: https://fle.github.io/detect-value-changes-between-successive-lines-with-postgresql.html\n",
    "\n",
    "# mark level, prev_level, next_level and level change label and always paid or always free label\n",
    "\n",
    "df_level = spark.sql(\"select userId, ttl_paid2free, ttl_free2paid, \\\n",
    "                      case when ttl_lvl_free == 0 then 1 else 0 end as always_paid, \\\n",
    "                      case when ttl_lvl_paid == 0 then 1 else 0 end as always_free \\\n",
    "                      from  \\\n",
    "                      (select userId, sum(paid2free) as ttl_paid2free, sum(free2paid) as ttl_free2paid, \\\n",
    "                      sum(level_paid) as ttl_lvl_paid, sum(level_free) as ttl_lvl_free \\\n",
    "                      from \\\n",
    "                      (select userId, date, level, prev_level, next_level, page, hour, \\\n",
    "                       case when (prev_level == 'paid' and level == 'free') then 1 else 0 end as paid2free, \\\n",
    "                       case when (prev_level == 'free' and level == 'paid') then 1 else 0 end as free2paid, \\\n",
    "                       case when (level == 'paid') then 1 else 0 end as level_paid, \\\n",
    "                       case when (level == 'free') then 1 else 0 end as level_free \\\n",
    "                       from \\\n",
    "                       (select userId, date, level, page, hour, \\\n",
    "                       lag(level) OVER (ORDER BY userId, date) as prev_level, \\\n",
    "                       lead(level) OVER (ORDER BY userId,date) as next_level \\\n",
    "                       from df_mini_non0_obv) as a) as b \\\n",
    "                       group by userId) as c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+-----------+-----------+\n",
      "|userId|ttl_paid2free|ttl_free2paid|always_paid|always_free|\n",
      "+------+-------------+-------------+-----------+-----------+\n",
      "|    10|            0|            0|          1|          0|\n",
      "|   100|            0|            0|          1|          0|\n",
      "|100002|            0|            0|          1|          0|\n",
      "|100004|            3|            2|          0|          0|\n",
      "|100007|            0|            1|          1|          0|\n",
      "+------+-------------+-------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_level.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method \n",
    "df_method = spark.sql(\"select distinct userId, method, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_method.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct method from df_mini_non0_obv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# registration -- drop since each user has an distinct regi number\n",
    "df_regi = spark.sql(\"select distinct userId, registration, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_regi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1619"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sessionId -- drop since too much level\n",
    "df_session = spark.sql(\"select distinct userId, sessionId, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_session.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# status -- need to figure out how the status changes for each user\n",
    "df_status = spark.sql(\"select distinct userId, status, churn_user_target from df_mini_non0_obv sort by userId\")\n",
    "df_status.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct status from df_mini_non0_obv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select distinct gender, state, opr system, total songs, pages,level-status and join them together\n",
    "\n",
    "df_fe = df_gender.join(df_location.select(['userId','state']), \"userId\")\\\n",
    "                 .join(df_useAgen.select(['userId','opr_system']), \"userId\")\\\n",
    "                 .join(df_song, \"userId\")\\\n",
    "                 .join(df_page, \"userId\")\\\n",
    "                 .join(df_song_length, \"userId\")\\\n",
    "                 .join(df_level, \"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------------+-----+----------+--------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+---------------+-------------+-------------+-----------+-----------+\n",
      "|userId|gender|churn_user_target|state|opr_system|ttl_song|About|Add Friend|Add to Playlist|Downgrade|Error|Help|Home|Logout|NextSong|Roll Advert|Save Settings|Settings|Submit Downgrade|Submit Upgrade|Thumbs Down|Thumbs Up|Upgrade|avg_song_length|ttl_paid2free|ttl_free2paid|always_paid|always_free|\n",
      "+------+------+-----------------+-----+----------+--------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+---------------+-------------+-------------+-----------+-----------+\n",
      "|100010|     F|                0|   CT|  iPhone; |     120|    0|         3|              2|        0|    0|   1|   6|     2|     120|         22|            0|       0|               0|             0|          1|        6|      1|          236.0|            1|            0|          0|          1|\n",
      "|200002|     M|                0|   WI|  iPhone; |     267|    2|         4|              6|        3|    0|   1|  14|     3|     267|          7|            0|       3|               0|             1|          6|       15|      2|          242.0|            1|            1|          0|          0|\n",
      "+------+------+-----------------+-----+----------+--------+-----+----------+---------------+---------+-----+----+----+------+--------+-----------+-------------+--------+----------------+--------------+-----------+---------+-------+---------------+-------------+-------------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fe.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace space with '_' in column names \n",
    "# https://stackoverflow.com/questions/41655158/dynamically-rename-multiple-columns-in-pyspark-dataframe\n",
    "col_replace = {c:c.replace(' ', '_') for c in df_fe.columns if ' ' in c}\n",
    "df_fe = df_fe.select([col(c).alias(col_replace.get(c,c)) for c in df_fe.columns])\n",
    "\n",
    "# create and temp view\n",
    "df_fe.createOrReplaceTempView(\"mini_fe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore difference between churn/stay user\n",
    "# https://stackoverflow.com/questions/1823599/calculating-percentage-within-a-group\n",
    "\n",
    "# add if categorical / continous\n",
    "def cunt_bychurn_vara(vara):\n",
    "    \"\"\" Show variable's unique count, and proportion by vara and churn/stay.\n",
    "    INPUT:\n",
    "    vara: -- (string), variable need to be counted.\n",
    "    OUTPUT:\n",
    "    df -- (dataframe), vara and corresponding proportion / avg value\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if categorical variable, if so then display proportions\n",
    "    if vara in [c[0] for c in df_fe.dtypes if c[1] == 'string']:\n",
    "        df = spark.sql(\"select {}, churn_user_target, count(*) as {}_cnt, \\\n",
    "                   round(count(*)/CAST( SUM(count(*)) over (partition by churn_user_target) as float), 2) as {}_pctg\\\n",
    "                   from mini_fe group by {}, churn_user_target \\\n",
    "                   order by  churn_user_target, {}\".format(vara,vara,vara,vara,vara))\n",
    "    # if is date then skip    \n",
    "    elif vara in [c[0] for c in df_fe.dtypes if c[1] == 'date']:\n",
    "        print('Input variable is date.')\n",
    "        \n",
    "    # if continous type like int, big int, float then display average between two groups\n",
    "    else: \n",
    "        df = spark.sql(\"select churn_user_target, round(avg({}),2) as avg_{}_mthly\\\n",
    "                   from mini_fe group by churn_user_target\".format(vara, vara))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+\n",
      "|churn_user_target|avg_ttl_paid2free_mthly|\n",
      "+-----------------+-----------------------+\n",
      "|                1|                    0.5|\n",
      "|                0|                   0.65|\n",
      "+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara('ttl_paid2free').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|churn_user_target|avg_always_free_mthly|\n",
      "+-----------------+---------------------+\n",
      "|                1|                 0.23|\n",
      "|                0|                 0.37|\n",
      "+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara('always_free').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+\n",
      "|churn_user_target|avg_ttl_free2paid_mthly|\n",
      "+-----------------+-----------------------+\n",
      "|                1|                   0.77|\n",
      "|                0|                    0.6|\n",
      "+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara('ttl_free2paid').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|churn_user_target|avg_always_paid_mthly|\n",
      "+-----------------+---------------------+\n",
      "|                1|                 0.09|\n",
      "|                0|                 0.18|\n",
      "+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara('always_paid').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------+-----------+\n",
      "|gender|churn_user_target|gender_cnt|gender_pctg|\n",
      "+------+-----------------+----------+-----------+\n",
      "|     F|                0|        77|       0.48|\n",
      "|     M|                0|        84|       0.52|\n",
      "|     F|                1|        11|        0.5|\n",
      "|     M|                1|        11|        0.5|\n",
      "+------+-----------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara('gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|churn_user_target|avg_Thumbs_Down_mthly|\n",
      "+-----------------+---------------------+\n",
      "|                1|                11.09|\n",
      "|                0|                 6.01|\n",
      "+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara(\"Thumbs_Down\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|churn_user_target|avg_ttl_song_mthly|\n",
      "+-----------------+------------------+\n",
      "|                1|             742.5|\n",
      "|                0|            548.88|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara(\"ttl_song\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|churn_user_target|avg_Thumbs_Up_mthly|\n",
      "+-----------------+-------------------+\n",
      "|                1|              37.18|\n",
      "|                0|              31.83|\n",
      "+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara(\"Thumbs_Up\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+\n",
      "|churn_user_target|avg_Submit_Downgrade_mthly|\n",
      "+-----------------+--------------------------+\n",
      "|                1|                      0.27|\n",
      "|                0|                      0.18|\n",
      "+-----------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara(\"Submit_Downgrade\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|churn_user_target|avg_Error_mthly|\n",
      "+-----------------+---------------+\n",
      "|                1|           0.41|\n",
      "|                0|           0.67|\n",
      "+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cunt_bychurn_vara(\"Error\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Plot variable distribution between churn/stay groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore difference between churn/stay user\n",
    "# https://stackoverflow.com/questions/1823599/calculating-percentage-within-a-group\n",
    "\n",
    "# add if categorical / continous\n",
    "def plot_churn_vara(vara):\n",
    "    \"\"\" Show variable's unique count, and proportion by vara and churn/stay.\n",
    "    INPUT:\n",
    "    vara: -- (string), variable need to be counted.\n",
    "    OUTPUT:\n",
    "    plot corresponding proportion / avg value by churn/stay\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert spark df to pandas\n",
    "    df_fe_pd = df_fe.toPandas()\n",
    "    \n",
    "    # check if categorical variable, if so then display proportions\n",
    "    if vara in [c[0] for c in df_fe.dtypes if c[1] == 'string']:\n",
    "        df_fe_pd.groupby(['churn_user_target',vara])['userId'].count().plot(kind = 'bar', color = 'g')\n",
    "        plt.ylabel('{}_count'.format(vara))\n",
    "        plt.show()    \n",
    "        \n",
    "    # if is date then skip    \n",
    "    elif vara in [c[0] for c in df_fe.dtypes if c[1] == 'date']:\n",
    "        print('Input variable is date.')\n",
    "        \n",
    "    # if continous type like int, big int, float then display average between two groups\n",
    "    else: \n",
    "        df_fe_pd.groupby(['churn_user_target'])[vara].mean().plot(kind = 'bar')\n",
    "        plt.ylabel('monthly_avg_{}'.format(vara))\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEeCAYAAABonHmPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGYNJREFUeJzt3XuUZWWd3vHvAw0iCMOtYCGIjQ6I6ChiBRhBw9URozaOohDjtEimFwmiokbQMSJOYtQYjOMt0w5qmyhXYWCICtgDRoi2dgNyFdtpEJEeaGa4CCpy+eWPvSsURVV1nepzqdP9/azV65x9O/tXp0+dp9537/3uVBWSpA3bRoMuQJI0eIaBJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJAHzBl3ATG2//fY1f/78QZchSUNlxYoV91TVyNrWG5owmD9/PsuXLx90GZI0VJL8Yibr2U0kSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiSG6ApkzW05LYMuYUbq1Bp0CdKcZMtAkmQYSJIMA0kShoEkCcNAkkQfwiDJSUluTHJDkjOTbJZktyTLkqxMcnaSTXtdhyRpaj0NgyQ7A+8ERqvqhcDGwNHAJ4BPV9XuwL3Acb2sQ5I0vX50E80Dnp5kHrA5sBo4BDivXb4EOLIPdUiSptDTMKiqXwGfAm6nCYH7gRXAfVX1aLvaHcDOvaxDkjS9XncTbQMsAHYDnglsARwxyaqTXhaaZFGS5UmWr1mzpneFStIGrtfdRIcBt1bVmqp6BDgfeBmwddttBLALcOdkG1fV4qoararRkZGRHpcqSRuuXofB7cD+STZPEuBQ4CbgcuCN7ToLgQt7XIckaRq9PmawjOZA8dXA9e3+FgMnA+9J8nNgO+CMXtYhSZpez0ctrapTgVMnzF4F7NvrfUuSZsYrkCVJhoEkaQO/uY03ZJGkhi0DSZJhIEkyDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kSPQ6DJM9Lcu24fw8keXeSbZNclmRl+7hNL+uQJE2v17e9vKWq9q6qvYGXAr8BLgBOAZZW1e7A0nZakjQg/ewmOhT4h6r6BbAAWNLOXwIc2cc6JEkT9DMMjgbObJ/vWFWrAdrHHSbbIMmiJMuTLF+zZk2fypSkDU9fwiDJpsDrgHM72a6qFlfVaFWNjoyM9KY4SVLfWgZHAFdX1V3t9F1JdgJoH+/uUx2SpEn0KwyO4YkuIoCLgIXt84XAhX2qQ5I0iZ6HQZLNgcOB88fN/jhweJKV7bKP97oOSdLU5vV6B1X1G2C7CfP+iebsIknSHOAVyJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJ/tzcZusk5yX5aZKbk/xxkm2TXJZkZfu4Ta/rkCRNrR8tg88A36mqPYEXAzcDpwBLq2p3YGk7LUkakJ6GQZKtgFcAZwBU1e+r6j5gAbCkXW0JcGQv65AkTa/XLYPnAGuAryS5JsnfJNkC2LGqVgO0jzv0uA5J0jR6HQbzgH2AL1bVS4CH6KBLKMmiJMuTLF+zZk2vapSkDV6vw+AO4I6qWtZOn0cTDncl2Qmgfbx7so2ranFVjVbV6MjISI9LlaQNV0/DoKr+Efhlkue1sw4FbgIuAha28xYCF/ayDknS9Ob1YR8nAl9PsimwCjiWJoTOSXIccDtwVB/qkCRNoedhUFXXAqOTLDq01/uWJM2MVyBLkgwDSVIHYZDkKf36k82TJA2fTloGH5jhPEnSkFnrAeQkRwCvBnZO8lfjFm0FPNqrwiRJ/TOTs4nuBJYDrwNWjJv/a+CkXhQlSeqvtYZBVf0E+EmSb1TVI32oSZLUZ51cZ7Bvko8Az263C1BV9ZxeFCZJ6p9OwuAMmm6hFcBjvSlHkjQInYTB/VX17Z5VIkkamE7C4PIk/xU4H3h4bGZVXd31qiRJfdVJGOzXPo4fZ6iAQ7pXjiRpEGYcBlV1cC8LkSQNzozDIMmHJ5tfVR/tXjmSpEHopJvooXHPNwNeA9zc3XIkSYPQSTfRfxs/neRTNHcskyQNuXW5uc3mwFovOEtyG83QFY8Bj1bVaJJtgbOB+cBtwJuq6t51qEWStA46GcL6+iTXtf9uBG4BPjPDzQ+uqr2rauxMpFOApVW1O7C0nZYkDUgnLYPXjHv+KHBXVc121NIFwEHt8yXAFcDJs3wtSdI6mnHLoKp+AWwNvBZ4PbDXTDcFLk2yIsmidt6OVbW6fd3VwA4zL1mS1G2ddBO9C/g6zRf3DsDXk5w4g00PqKp9gCOAE5K8ooN9LkqyPMnyNWvWzHQzSVKHOrnT2XHAflX14ar6MLA/8Odr26iq7mwf7wYuAPYF7kqyE0D7ePcU2y6uqtGqGh0ZGemgVElSJzoJg/Dk0Uofa+dNvUGyRZItx54DrwRuoDkldWG72kLgwg7qkCR1WScHkL8CLEtyQTt9JM2w1tPZEbggydi+vlFV30nyY+CcJMcBtwNHdVa2JKmbOrno7PQkVwAH0rQIjq2qa9ayzSrgxZPM/yfg0M5KlST1SidjE+0P3Dg2ZHWSLZPsV1XLeladJKkvOjlm8EXgwXHTD7XzJElDrqMDyFVVYxNV9TjrNpyFJGmO6CQMViV5Z5JN2n/vAlb1qjBJUv90EgbHAy8DfgXcQXPns0XTbiFJGgqdnE10N3D0VMuTfKCq/ktXqpIk9VUnLYO18VoBSRpS3QyDaa9GliTNXd0Mg1r7KpKkuciWgSRpZmGQZOMkJ61ltXO7UI8kaQBmFAZV9RjN3cmmW+djXalIktR3nVxBfFWSz9HcyP6hsZljYxVJkoZXJ2Hwsvbxo+PmFXBI98qRJA1CJxedHdzLQiRJg9PJPZB3THJGkm+303u1N6eRJA25Tk4t/SpwCfDMdvpnwLtnsmF7NtI1SS5up3dLsizJyiRnJ9m0k6IlSd3VSRhsX1XnAI8DVNWjPPmeyNN5F3DzuOlPAJ+uqt2BewFbGJI0QJ2EwUNJtqO90ri989n9a9soyS7AvwL+pp0OzUHn89pVltDcT1mSNCCdnE30HuAi4LlJrgJGgDfOYLv/Drwf2LKd3g64r21ZQDMc9s6TbZhkEe0w2bvuumsHpUqSOtHJ2URXJ/mXwPNohp64paoemW6bJK8B7q6qFUkOGps92ctPsc/FwGKA0dFRxz6SpB5Zaxgk+dMpFu2RhKo6f5rNDwBel+TVwGbAVjQtha2TzGtbB7sAd3ZYtySpi2bSMnht+7gDzYVnf99OHwxcAUwZBlX1AeADAG3L4H1V9ZYk59J0MZ0FLAQunEXtkqQuWesB5Ko6tqqOpenK2auq3lBVbwBesA77PRl4T5Kf0xxDOGMdXkuStI46OYA8v6pWj5u+C9hjphtX1RU0LQmqahWwbwf7liT1UCdhcEWSS4AzaVoJRwOX96QqSVJfdXI20Tvag8kvb2ctrqoLelOWJKmfOmkZjJ05NN3ZQ5KkIdTJQHV/2o4ldH+SB5L8OskDvSxOktQfnbQMPgm8tqpuXuuakqSh0snYRHcZBJK0fuqkZbA8ydnA3wIPj81cyxXIkqQh0EkYbAX8BnjluHmFB5Qlaeh1cmrpsb0sRJI0OJ2cTbRHkqVJbminX5TkQ70rTZLUL50cQP4SzaBzjwBU1XU0VyFLkoZcJ2GweVX9aMK8RyddU5I0VDoJg3uSPJcnbnv5RmD19JtIkoZBJ2cTnUBz17E9k/wKuBV4S0+qkiT1VSdhcCTwLZqRSjcCHgIOS7Kiqq7tRXGSpP7opJtoFDge2AbYmuZG9QcBX0ry/sk2SLJZkh8l+UmSG5Oc1s7fLcmydqyjs5Nsum4/hiRpXXQSBtsB+1TV+6rqvTThMAK8AnjbFNs8DBxSVS8G9gZelWR/4BPAp6tqd+Be4LhZ1i9J6oJOwmBX4Pfjph8Bnl1Vv2Xc8BTjVePBdnKT9l8BhwDntfOX0HRBSZIGpJNjBt8Afphk7Ob1rwXOTLIFcNNUGyXZGFgB/CHweeAfgPuqauy01DuAnTstXJLUPZ0MR/GXSb4FHAgEOL6qlreLpzyrqKoeA/ZOsjVwAfD8yVabbNski2iOTbDrrrvOtFRJUoc6vdPZCpq/8jtWVfcluQLYH9g6yby2dbALcOcU2yymOZ2V0dHRSQNDkrTuOjlm0LEkI22LgCRPBw4DbqY5PfWN7WoLgQsnfwVJUj901DKYhZ2AJe1xg42Ac6rq4iQ3AWcl+U/ANcAZPa5DkjSNnoZBO5jdSyaZvwrYt5f7liTNXE+7iSRJw8EwkCQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn0/raXz0pyeZKbk9yY5F3t/G2TXJZkZfu4TS/rkCRNr9ctg0eB91bV84H9gROS7AWcAiytqt2Bpe20JGlAehoGVbW6qq5un/8auBnYGVgALGlXWwIc2cs6JEnT69sxgyTzae6HvAzYsapWQxMYwA79qkOS9FR9CYMkzwC+Cby7qh7oYLtFSZYnWb5mzZreFShJG7ieh0GSTWiC4OtVdX47+64kO7XLdwLunmzbqlpcVaNVNToyMtLrUiVpg9Xrs4kCnAHcXFWnj1t0EbCwfb4QuLCXdUiSpjevx69/APBW4Pok17bzPgh8HDgnyXHA7cBRPa5DkjSNnoZBVV0JZIrFh/Zy35KkmfMKZEmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJJE7+909uUkdye5Ydy8bZNclmRl+7hNL2uQJK1dr1sGXwVeNWHeKcDSqtodWNpOS5IGqNd3Ovs/SeZPmL0AOKh9vgS4Aji5l3VIwyanTXWDwLmlTq1BlzAjvp9rN4hjBjtW1WqA9nGHAdQgSRpnTh9ATrIoyfIky9esWTPociRpvTWIMLgryU4A7ePdU61YVYurarSqRkdGRvpWoCRtaAYRBhcBC9vnC4ELB1CDJGmcXp9aeibwA+B5Se5IchzwceDwJCuBw9tpSdIA9fpsomOmWHRoL/crSerMnD6ALEnqD8NAkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJDHAMEjyqiS3JPl5klMGVYckaUBhkGRj4PPAEcBewDFJ9hpELZKkwbUM9gV+XlWrqur3wFnAggHVIkkbvEGFwc7AL8dN39HOkyQNwLwB7TeTzKunrJQsAha1kw8muaWnVXXH9sA93XzBfGSyt2uD4HvZXb6f3TUs7+ezZ7LSoMLgDuBZ46Z3Ae6cuFJVLQYW96uobkiyvKpGB13H+sD3srt8P7trfXs/B9VN9GNg9yS7JdkUOBq4aEC1SNIGbyAtg6p6NMk7gEuAjYEvV9WNg6hFkjS4biKq6lvAtwa1/x4aqm6tOc73srt8P7trvXo/U/WU47aSpA2Mw1FIkgwDSdIAjxmsT5JsAzwT+C1wW1U9PuCShl6SLYDfVdVjg65lWCXZCHgxT3w2b6yquwZb1XBbn3/XPWYwS0n+ADgBOAbYFFgDbAbsCPwQ+EJVXT64CodL+8V1NPAW4F8ADwNPo3lfvwUsrqqVg6tweCR5LnAycBiwkic+m3sAvwH+GliyPn2R9dKG8rtuGMxSksuArwF/V1X3TVj2UuCtwPVVdcYg6hs2Sb4HfBe4ELhh7IsqybbAwcC/Bi6oqv81uCqHQ5IzgS8C368Jv+BJdqB5L++tqiWDqG/YbCi/64aB5oQkm1TVI+u6jqTZMQxmKck7qupz7fMXeNHcumlbAFOqqn/uVy3DLsnHquqD7fPDq+qyQdc0zJLsM93yqrq6X7X0kmEwS0murqp9Jj7X7CR5nGbMqkfHZo1bXFX1nP5XNZz8bHZX+9m8keZYATz1s3lI/6vqPs8m6o4NeujGLvkscBBwFXAmcOXE/m5pQN4LvIHmDKKzaI5dPTjYkrrPlsEsJVlF8yHZCPgk8B/GL6+q8wdR1zBLEppAOIbmBkiXAl+sqlsHWdewSXIHcDrNHykntc//v6o6fbLtNL0ku9F8NhcAvwA+VlXXDraq7jEMZinJV6ZZXFX19r4Vs55JsjXNaaZ/CXywqr404JKGSpJTp1teVaf1q5b1TZIX0Hw23wq8v6rOGXBJXWMYaE5oLzJbALwZGAHOB86uql9Ou6HUY0meQxMAC2ju0HgWcHFV/W6ghXWZYaA5IclDNBdInQn8nAl3vrPbTYPSHkC+juYamAd46mdzveh28wCy5opzaX7J9mz/jVc0LQVpED7KEwHwjEEW0ku2DCRJjlrabUlGk+w86DqkiZIsSLLfoOvQ3GQ3UfedCLwoyc+q6s2DLkYaZz/gj5LMq6ojBl2M5ha7iXokyZZV9etB1yFJM2EYrIN2aNtXATvTHGC6E7hk4siGmr0ko8DqqvrVoGtZHzhWUfckWQD8Y1UtG3Qt3eAxg1lK8mfA1TRXzG4ObEEz1PKKdpm640Tg4iRnD7qQ9cRQD7M8x+wHfCjJtwddSDfYMpilJLcA+00yvvk2wLKq2mMwla2f7HabuSQXTbUIOKSqtuhnPRoOHkCevTDh4pPW4zhw3axM1+1mEHTk5cC/ASYOphaaMZ/UJetTt5thMHv/Gbg6yaU0l6gD7AocTjOmjjrQdq2dSjM43djxgYOBjyU5raq+NrDihs8Pgd9U1fcmLmhbtOqeM2h+74ee3UTroO0S+hOav2RDMx7/JVV170ALG0J2u2mu2lC63WwZzFKStF/6Z61lHdN2Zux265KZfO78bHZkg+h2Mwxm7/Ik3wQurKrbx2Ym2RQ4EFgIXA58dTDlDR273brHz2Z3bRDdbnYTzVKSzYC3A28BdgPuA55Oc7rupcDn16cbX/SD3W7dMcVnczNgY/xsagqGQRck2QTYHvitF5zNjl0bveFnc91tKJ9NLzrrgqp6pKpW+8u2Ti5PcmKSJ52ZkWTTJIckWULTvaEO+Nnsig3is2nLQHOC3W6aqzaUbjfDQHOOXRuaq9bnz6ZhIEnymIEkyTCQJGEYSJIwDNQjSb6a5I2DrqMXkhyU5GV92te7k2zej32N2+fbknyun/vU4BkGmpOSbDwHaphquJaDgI7CYJrXWpt309w8ac5ah59Nc4hhoK5I8mdJrkvykyT/s539iiT/N8mqsVZC+1f1xeO2+1ySt7XPb0vy4SRXAkcluSLJJ5L8KMnPkrx8mv0/6a/ZJBe3+9q4baXckOT6JCe1y5+b5DtJViT5fpI92/lfTXJ6ksuBT0yyn/nA8cBJSa5N8vIkr02yLMk1Sb6bZMd23Y8kWdyOt/S1JJsnOad9n85utxlt131lkh8kuTrJuUmekeSdwDNpLnq6fJJaXp3kp0muTPJXY+9rki2SfDnJj9uaFox7j85vf+6VST457rWObd/j7wEHjJs/kuSb7Wv9OMkBk/1sU/2/aHiY6FpnSV4A/AVwQFXdk2Rb4HRgJ5qB0fYELgLOm8HL/a6qDmxf93hgXlXtm+TVNPc7OKzD8vYGdq6qF7avuXU7fzFwfFWtTLIf8AXgkHbZHsBhVfXYxBerqtuS/A/gwar6VPua2wD7V1Ul+bfA+4H3tpu8FDiwqn6b5H3AvVX1oiQvBK5tt98e+FC7z4eSnAy8p6o+muQ9wMFVdc/4OtoLof4aeEVV3ZrkzHGL/wL4+6p6e/vz/ijJd8e9Hy8BHgZuSfJZ4FHgtLbW+2kGsbumXf8zwKer6sr2CtxLgOdP/Nmm+w/QcDAM1A2HAOeNfWFV1T8nAfjbqnocuGnsr+UZmHiv4/PbxxXA/FnUtgp4Tvul97+BS5M8g6ab59y2ToCnjdvm3MmCYBq7AGcn2QnYFLh13LKLxn1ZHkjz5UpV3ZDkunb+/sBewFVtPZsCP1jLPvcEVlXV2L7OBBa1z18JvK4NH2iulh0bSmFpVd0PkOQm4Nk0F1FdUVVr2vln0wQiNOG717j3aaskW07ys2nIGQbqhqnuRfDwhHWg+St0fPfkZhO2eWiK13iM6T+vk75uVd2b5MU0o6GeALyJph/+vqrae4rXmljD2nwWOL2qLkpyEPCRKV5rqvsyBLisqo7pYJ/T3eMhwBuq6knDK7ctoPH/J+Pf06muPt0I+OOJX/ptOHT6PmkO85iBumEp8KYk2wG03URT+QXNX5pPS3PP40O7VMNtwN5JNkryLNqbjrRdMBtV1TeB/wjsU1UPALcmOapdJ21gzNSvgS3HTf8BT9yqc7oBy66kCSOS7AX8UTv/h8ABSf6wXbZ5krG/zJ+0ryRfS7Iv8FOaFs/8dtGbx+3nEuDEtN/YSV6ylp9nGXBQku3SDLdw1LhllwLvGLf/qQJUQ84w0Dqrqhtpbk7zvSQ/oTleMNW6vwTOAa4Dvs4TfdPr6iqa7pnrgU8BV7fzdwauSHItzc1cPtDOfwtwXFvvjcCCDvb1d8Drxw4g07QEzk3yfeCeabb7AjDSdg+dTPMe3N92z7wNOLNd9kOabiBojm18e9wB5BcBq9u/1P898J00B9zvounvh+ZmQJsA1yW5gbXcHKiqVrc/ww+A7/LEewfwTmC0Peh9E83Bc62HHJtI6pM0p8tuUlW/S/JcmhbVHlX1+xluvxVwRlWNtWieUVUPti2AzwMrq+rTvapf6zePGUj9sznNaaKb0PTr/7uZBgFA2701vgvnz5MspDngfA3N2UXSrNgy0FBJ8ic89fz/W6vq9T3Y17HAuybMvqqqTuj2vqRBMwwkSR5AliQZBpIkDANJEoaBJAnDQJIE/D8U8MHoV2OTtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_churn_vara('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_churn_vara('ttl_song')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_churn_vara('Thumbs_Down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_churn_vara('Downgrade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_churn_vara('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Now we get cleaned user-feature dataset with each row represents an unique user and each feature shows their demographics or aggregation activities of last month. But before fitting into machine learning model, we need to do final data processing. We need to convert categorical features into dummy values and scale continuous features.\n",
    "\n",
    "**Categorical Features**\n",
    "\n",
    "First, we need to use StringIndexer to index categorical values. Then we will use OneHotEncoder to convert the indexer into dummy vectors.\n",
    "\n",
    "**Continuous Features**\n",
    "Scale the numerical data before fitting into machine learning models. StandardScaler will be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('churn_user_target', 'int'),\n",
       " ('state', 'string'),\n",
       " ('opr_system', 'string'),\n",
       " ('ttl_song', 'bigint'),\n",
       " ('About', 'bigint'),\n",
       " ('Add_Friend', 'bigint'),\n",
       " ('Add_to_Playlist', 'bigint'),\n",
       " ('Downgrade', 'bigint'),\n",
       " ('Error', 'bigint'),\n",
       " ('Help', 'bigint'),\n",
       " ('Home', 'bigint'),\n",
       " ('Logout', 'bigint'),\n",
       " ('NextSong', 'bigint'),\n",
       " ('Roll_Advert', 'bigint'),\n",
       " ('Save_Settings', 'bigint'),\n",
       " ('Settings', 'bigint'),\n",
       " ('Submit_Downgrade', 'bigint'),\n",
       " ('Submit_Upgrade', 'bigint'),\n",
       " ('Thumbs_Down', 'bigint'),\n",
       " ('Thumbs_Up', 'bigint'),\n",
       " ('Upgrade', 'bigint'),\n",
       " ('avg_song_length', 'double'),\n",
       " ('ttl_paid2free', 'bigint'),\n",
       " ('ttl_free2paid', 'bigint'),\n",
       " ('always_paid', 'int'),\n",
       " ('always_free', 'int')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Dealing with Categorical Variables\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# collect categorical variables\n",
    "cate_cols = [c[0] for c in df_fe.dtypes if (c[1] == 'string' and c[0] not in ['userId','state'])]\n",
    "\n",
    "# pipeline stages\n",
    "stages = []\n",
    "\n",
    "for cat in cate_cols:\n",
    "    # Indexing on categorical variables\n",
    "    string_indexer = StringIndexer(inputCol = cat, outputCol = cat + \"Index\")\n",
    "    \n",
    "    # OneHotEncoding on categorical variables to dummy vectors\n",
    "    dum_encoder = OneHotEncoder(inputCol = string_indexer.getOutputCol(), outputCol = cat + \"classVec\")\n",
    "    \n",
    "    stages += [string_indexer, dum_encoder]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with numerical variables\n",
    "# https://blog.usejournal.com/tutorial-on-pyspark-transformations-and-mlib-7ed289a9e843\n",
    "\n",
    "# collect continous variable\n",
    "conti_cols = [c[0] for c in df_fe.dtypes if c[1] in ('int','bigint','double') and c[0] != 'churn_user_target']\n",
    "\n",
    "# assemble continous variables\n",
    "conti_assemb = VectorAssembler(inputCols = conti_cols, outputCol = \"conti_varas_vec\")\n",
    "cat_scaler = StandardScaler(inputCol = \"conti_varas_vec\", outputCol = \"conti_scaled_feats\")\n",
    "stages += [conti_assemb, cat_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conver target variable into label indices\n",
    "label_strIdx = StringIndexer(inputCol = \"churn_user_target\", outputCol = \"label\")\n",
    "stages += [label_strIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assemble vectors together\n",
    "assembler_varas = [c + \"Index\" for c in cate_cols] + [\"conti_scaled_feats\"]\n",
    "# Apply vectorAssembler\n",
    "assembler = VectorAssembler(inputCols = assembler_varas, outputCol = \"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Prediction Model\n",
    "model = RandomForestClassifier(featuresCol = \"features\", labelCol = \"label\", numTrees = 10)# LogisticRegression(featuresCol = \"features\", labelCol = \"label\")#\n",
    "stages += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile pipeline\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For next part using convenience, Let's create a pipeline function\n",
    "\n",
    "def ml_pipeline(df, model):\n",
    "    \"\"\"Build feature vector processing and ML pipeline for prediction ready.\n",
    "    INPUT:\n",
    "    df -- (dataframe): Dataframe with users as rows and features as columns.\n",
    "    model : machine learning alogrithm.\n",
    "    OUTPUT:\n",
    "    pipeline -- (Pipeline): Feature processing and ML pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Dealing with Categorical Variables\n",
    "    # collect categorical variables\n",
    "    cate_cols = [c[0] for c in df.dtypes if (c[1] == 'string' and c[0] not in ['userId','state'])]\n",
    "    # pipeline stages\n",
    "    stages = []\n",
    "\n",
    "    for cat in cate_cols:\n",
    "        # Indexing on categorical variables\n",
    "        string_indexer = StringIndexer(inputCol = cat, outputCol = cat + \"Index\")\n",
    "        # OneHotEncoding on categorical variables to dummy vectors\n",
    "        dum_encoder = OneHotEncoder(inputCol = string_indexer.getOutputCol(), outputCol = cat + \"classVec\")\n",
    "        # update stage\n",
    "        stages += [string_indexer, dum_encoder]\n",
    "\n",
    "    ### Dealing with numerical variables\n",
    "    # collect continous variable\n",
    "    conti_cols = [c[0] for c in df.dtypes if c[1] in ('int','bigint','double') and c[0] != 'churn_user_target']\n",
    "    # assemble continous variables\n",
    "    conti_assemb = VectorAssembler(inputCols = conti_cols, outputCol = \"conti_varas_vec\")\n",
    "    # scale continous variables\n",
    "    cat_scaler = StandardScaler(inputCol = \"conti_varas_vec\", outputCol = \"conti_scaled_feats\")\n",
    "    # update satage\n",
    "    stages += [conti_assemb, cat_scaler]\n",
    "    \n",
    "    \n",
    "    ### conver target variable into label indices\n",
    "    label_strIdx = StringIndexer(inputCol = \"churn_user_target\", outputCol = \"label\")\n",
    "    # update stage\n",
    "    stages += [label_strIdx]\n",
    "    \n",
    "    # Assemble vectors together\n",
    "    assembler_varas = [c + \"Index\" for c in cate_cols] + [\"conti_scaled_feats\"]\n",
    "    # Apply vectorAssembler\n",
    "    assembler = VectorAssembler(inputCols = assembler_varas, outputCol = \"features\")\n",
    "    # Update stage\n",
    "    stages += [assembler]\n",
    "    \n",
    "    ### ML model\n",
    "    ml_model  = model\n",
    "    stages += [ml_model]\n",
    "    \n",
    "    ### Compile pipeline\n",
    "    pipeline = Pipeline().setStages(stages)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "\n",
    "1. Initialize the model will be used in the model building.\n",
    "2. Call pipeline by feeding feature engineered dataset and model.\n",
    "3. Split train test dataset.\n",
    "4. Set paramGrid search candidates and use TrainValidationSplit to split trani and validation set to evaluate model.\n",
    "5. Extract the best model from the parameter search from last step and do prediction on the test set.\n",
    "6. Also use F1 metric to evaluate predicted performance.\n",
    "7. Try Random Forest and LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(featuresCol = \"features\", labelCol = \"label\")\n",
    "pipeline = ml_pipeline(df_fe, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test \n",
    "train, test = df_fe.randomSplit([0.8, 0.2], seed = 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(model.numTrees, [10, 20]) \\\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-validation for finding the best param \n",
    "train_val = TrainValidationSplit(estimator = pipeline,\n",
    "                           estimatorParamMaps = paramGrid,\n",
    "                           evaluator = BinaryClassificationEvaluator(),\n",
    "                           trainRatio = 0.7)\n",
    "\n",
    "ml_model = train_val.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5625000000000002, 0.7916666666666667]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model_best = ml_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test result\n",
    "result = ml_model_best.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6578947368421053"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on AUC score\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "evaluator.evaluate(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compile above steps together into a model contruct function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_contruct(df, model, paramGrid, evaluator):\n",
    "    \"\"\"Split train, test dataset. Grid Search best model by using TrainValidationSplit and do prediction on test set.\n",
    "    \n",
    "    INPUT:\n",
    "    df -- (dataframe): Dataframe with users as rows and features as columns.\n",
    "    model : machine learning alogrithm.\n",
    "    paramGrid: parameters for grid search.\n",
    "    evaluator : The evaluation method.\n",
    "    \n",
    "    OUTPUT:\n",
    "    evaluator_val: model performance on validation dataset.\n",
    "    evaluator_test: model performance on test dataset.\n",
    "    \"\"\"\n",
    "    # build data process ml pipeline \n",
    "    pipeline = ml_pipeline(df, model)\n",
    "    \n",
    "    # split train test \n",
    "    train, test = df.randomSplit([0.8, 0.2], seed = 41)\n",
    "    \n",
    "    # split train-validation for finding the best param \n",
    "    train_val_model = TrainValidationSplit(estimator = pipeline,\n",
    "                               estimatorParamMaps = paramGrid,\n",
    "                               evaluator = evaluator,\n",
    "                               trainRatio = 0.7)\n",
    "    \n",
    "    # train model\n",
    "    ml_model = train_val_model.fit(train)\n",
    "    \n",
    "    # get each parameter setting's evaluation result list\n",
    "    evaluator_val = ml_model.validationMetrics\n",
    "    \n",
    "    # extract best model\n",
    "    ml_model_best = ml_model.bestModel\n",
    "    \n",
    "    # test result\n",
    "    result = ml_model_best.transform(test)\n",
    "    \n",
    "    # set evaluator\n",
    "    evaluator = evaluator\n",
    "    \n",
    "    # get evaluation result\n",
    "    evaluator_test = evaluator.evaluate(result)\n",
    "    \n",
    "    return evaluator_val, evaluator_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try random forest \n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = \"features\", labelCol = \"label\")#LogisticRegression(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10, 20]) \\\n",
    "        .build()\n",
    "\n",
    "rf_val_result, rf_test_result = model_contruct(df_fe, rf, paramGrid, BinaryClassificationEvaluator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5625000000000002, 0.7916666666666667]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8552631578947368"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then try Logistic Regression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = \"features\", labelCol = \"label\")#LogisticRegression(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.maxIter, [10, 20]) \\\n",
    "        .build()\n",
    "\n",
    "lr_val_result, lr_test_result = model_contruct(df_fe, lr, paramGrid, BinaryClassificationEvaluator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8402777777777778, 0.9027777777777778]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8289473684210527"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on F1 Score\n",
    "lr = LogisticRegression(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.maxIter, [10, 20]) \\\n",
    "        .build()\n",
    "\n",
    "lr_val_result, lr_test_result = model_contruct(df_fe, lr, paramGrid, \\\n",
    "                                               MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9040000000000001, 0.9201565557729942]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9129870129870131"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "The AUC score of random forest and Logistic Regression predictions on test set are both above 0.8. which is pretty good. F1 score of logistic Regression is abot 0.91, which is also good. Considering the performance stable on both validation and testing set, Logitic Regression's performance is better than Random Forest. I think it might due to the high imbalance on the target distribution, which Logistic Regression could do a good job.\n",
    "\n",
    "However, due to the limited size of this dataset (only about 200 users included), the result might be very unstable if we change train, test dataset. \n",
    "\n",
    "**To be Improvemed**\n",
    "1. Re-analyze the whole process on the full size dataset. The whole dataset user log scenario might be very different with the small one. \n",
    "2. Set spark cluster on AWS EMR, which allow to access the full dataset in S3 and alos could boost calculation speed.\n",
    "3. Continue to extract more useful features and try more algorithms and parameter tunings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "\n",
    "After above exploratory I will compile all the necessary steps together for future scaling using.\n",
    "Include:\n",
    "* Data Loading and simple cleaning --    `data_prep(data_path)`\n",
    "* Data Transformation --    `feature_engineer(df)`\n",
    "* Feature Processing and ML pipline --   `ml_pipeline(df, model)`\n",
    "* Model Tranining and Predition --   `model_contruct(df_fe,model,paramGrid,evaluator)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Data Loading and simple cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_path):\n",
    "    \"\"\"Load raw log date of sparkify and define churn, separate observation and target data.\n",
    "    INPUT: data_path -- (str) : location of the file\n",
    "    OUTPUT: df_prep -- (dataframe): dataframe after cleaning null userIds\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Load raw data\n",
    "    # read raw data\n",
    "    df = spark.read.json(data_path)\n",
    "    # Filter out userId is null records\n",
    "    df = df.filter(df.userId != '')\n",
    "    \n",
    "    \n",
    "   ### Define churn \n",
    "\n",
    "    # define churn function -- user cancellation confirmation as criteria\n",
    "    cancellation_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "    # apply churn function\n",
    "    df = df.withColumn(\"churn\", cancellation_event(\"page\"))   \n",
    "    # fill churn user na as 0s\n",
    "    df = df.na.fill({'churn': 0})\n",
    "    \n",
    "    ### Generate time related variable\n",
    "    # get hour, day, date from ts\n",
    "    ts_to_hour = udf(lambda x: datetime.fromtimestamp(x / 1000.0).hour)\n",
    "    df = df.withColumn(\"hour\",ts_to_hour(\"ts\"))\n",
    "    ts_to_day = udf(lambda x: datetime.fromtimestamp(x / 1000.0).day)\n",
    "    df = df.withColumn(\"day\",ts_to_day(\"ts\"))\n",
    "    df = df.withColumn(\"date\",from_unixtime(df.ts/1000).cast(DateType()))\n",
    "    \n",
    "    ### segment obersavation and target data\n",
    "    # create sql view\n",
    "    df.createOrReplaceTempView(\"whole_data\")      \n",
    "    ## obersavation\n",
    "    # set observation records\n",
    "    df_obv = spark.sql(\"select * from whole_data \\\n",
    "                        where date >= '2018-10-01' and date < '2018-11-01'\")\n",
    "    # get unique churn users\n",
    "    churn_user_obv = df_obv.select(['userId']).where(df_obv.churn == 1).dropDuplicates()\n",
    "    # assign new churn user label to churn users\n",
    "    churn_user_obv = churn_user_obv.withColumn(\"churn_user_obv\", lit(1))\n",
    "    # join churn user back to original table and got an label\n",
    "    df_obv = df_obv.join(churn_user_obv, \"userId\", how = 'outer')\n",
    "    # remove users records who churn between 2018-10-01 and 2018-11-01\n",
    "    df_obv = df_obv.where(col('churn_user_obv').isNull())\n",
    "    \n",
    "    ## target\n",
    "    df_target = spark.sql(\"select * from whole_data \\\n",
    "                          where date >= '2018-11-01' and date < '2018-12-01'\")\n",
    "    # unique users churned during the target period\n",
    "    churn_user_target = df_target.select(['userId']).where(df_target.churn == 1).dropDuplicates()\n",
    "    # assign new churn user label to churn users\n",
    "    churn_user_target = churn_user_target.withColumn(\"churn_user_target\", lit(1))\n",
    "    \n",
    "    ## join observation and target\n",
    "    # join churn user back to observal table and got an label\n",
    "    df_prep = df_obv.join(churn_user_target, \"userId\", how = 'outer')\n",
    "\n",
    "    # fill na with 0s in the observational dataset\n",
    "    df_prep = df_prep.fillna(0, subset = ['churn_user_target'])\n",
    "\n",
    "    return df_prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    \"\"\"Data transformation and feature processing from loaded churn defined raw data \n",
    "       into model ready dataset.\n",
    "    INPUT: \n",
    "    df -- (dataframe): Loaded and churn defined raw datafrme\n",
    "    \n",
    "    OUTPUT: \n",
    "    df_fe -- (dataframe): Processed dataframe with users as rows and features as columns.\n",
    "    \"\"\"\n",
    "    # create temp view\n",
    "    df.createOrReplaceTempView(\"whole_data\")     \n",
    "    \n",
    "    ## gender by userId\n",
    "    df_gender = spark.sql(\"select distinct userId, gender, churn_user_target from whole_data\")\n",
    "    \n",
    "    ## location by userId\n",
    "    df_location = spark.sql(\"select distinct userId, location, churn_user_target from whole_data\")\n",
    "    # extract State -- 37\n",
    "    substr_state = udf(lambda x: x[-2:], StringType())\n",
    "    df_location = df_location.withColumn(\"state\", substr_state(\"location\"))\n",
    "    \n",
    "    ## userAgent\n",
    "    df_useAgen = spark.sql(\"select distinct userId, userAgent, churn_user_target from whole_data\")\n",
    "    remove_qot = udf(lambda x: x.replace(u'\"',''))\n",
    "    substr_agent = udf(lambda x: x[13:21], StringType())\n",
    "    # extract oprating system\n",
    "    df_useAgen = df_useAgen.withColumn(\"opr_system\", substr_agent(remove_qot(\"userAgent\")))    \n",
    "\n",
    "    ## Total songs played during the month\n",
    "    df_song = spark.sql(\"select userId, count(song) as ttl_song\\\n",
    "                         from whole_data \\\n",
    "                         where song is not Null \\\n",
    "                         group by userId\")\n",
    "    \n",
    "    ## Pages\n",
    "    # reshap page count -- userId as row and page as column, count as value\n",
    "    df_page = df.groupby('userId').pivot('page').count()\n",
    "    # fillna as 0s\n",
    "    df_page = df_page.fillna(0)\n",
    "    \n",
    "    ## Song length\n",
    "    df_song_length = spark.sql(\"select userId, round(avg(length)) as avg_song_length\\\n",
    "                            from whole_data \\\n",
    "                            where song is not Null \\\n",
    "                            group by userId\")\n",
    "    \n",
    "    # https://fle.github.io/detect-value-changes-between-successive-lines-with-postgresql.html\n",
    "\n",
    "    # mark level, prev_level, next_level and level change label\n",
    "    df_level = spark.sql(\"select userId, ttl_paid2free, ttl_free2paid, \\\n",
    "                          case when ttl_lvl_free == 0 then 1 else 0 end as always_paid, \\\n",
    "                          case when ttl_lvl_paid == 0 then 1 else 0 end as always_free \\\n",
    "                          from  \\\n",
    "                          (select userId, sum(paid2free) as ttl_paid2free, sum(free2paid) as ttl_free2paid, \\\n",
    "                          sum(level_paid) as ttl_lvl_paid, sum(level_free) as ttl_lvl_free \\\n",
    "                          from \\\n",
    "                          (select userId, date, level, prev_level, next_level, page, hour, \\\n",
    "                           case when (prev_level == 'paid' and level == 'free') then 1 else 0 end as paid2free, \\\n",
    "                           case when (prev_level == 'free' and level == 'paid') then 1 else 0 end as free2paid, \\\n",
    "                           case when (level == 'paid') then 1 else 0 end as level_paid, \\\n",
    "                           case when (level == 'free') then 1 else 0 end as level_free \\\n",
    "                           from \\\n",
    "                           (select userId, date, level, page, hour, \\\n",
    "                           lag(level) OVER (ORDER BY userId, date) as prev_level, \\\n",
    "                           lead(level) OVER (ORDER BY userId,date) as next_level \\\n",
    "                           from whole_data) as a) as b \\\n",
    "                           group by userId) as c\")\n",
    "    \n",
    "    # join above features together\n",
    "    df_fe = df_gender.join(df_location.select(['userId','state']), \"userId\")\\\n",
    "                     .join(df_useAgen.select(['userId','opr_system']), \"userId\")\\\n",
    "                     .join(df_song, \"userId\")\\\n",
    "                     .join(df_page, \"userId\")\\\n",
    "                     .join(df_song_length, \"userId\")\\\n",
    "                     .join(df_level, \"userId\")\n",
    "    \n",
    "    # replace space with '_' in column names \n",
    "    col_replace = {c:c.replace(' ', '_') for c in df_fe.columns if ' ' in c}\n",
    "    df_fe = df_fe.select([col(c).alias(col_replace.get(c,c)) for c in df_fe.columns])\n",
    "    \n",
    "    return df_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Feature Processing and ML pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comple ML pipeline\n",
    "\n",
    "def ml_pipeline(df, model):\n",
    "    \"\"\"Build feature vector processing and ML pipeline for prediction ready.\n",
    "    INPUT:\n",
    "    df -- (dataframe): Dataframe with users as rows and features as columns.\n",
    "    model : machine learning alogrithm.\n",
    "    OUTPUT:\n",
    "    pipeline -- (Pipeline): Feature processing and ML pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Dealing with Categorical Variables\n",
    "    # collect categorical variables\n",
    "    cate_cols = [c[0] for c in df.dtypes if (c[1] == 'string' and c[0] not in ['userId','state'])]\n",
    "    # pipeline stages\n",
    "    stages = []\n",
    "\n",
    "    for cat in cate_cols:\n",
    "        # Indexing on categorical variables\n",
    "        string_indexer = StringIndexer(inputCol = cat, outputCol = cat + \"Index\")\n",
    "        # OneHotEncoding on categorical variables to dummy vectors\n",
    "        dum_encoder = OneHotEncoder(inputCol = string_indexer.getOutputCol(), outputCol = cat + \"classVec\")\n",
    "        # update stage\n",
    "        stages += [string_indexer, dum_encoder]\n",
    "\n",
    "    ### Dealing with numerical variables\n",
    "    # collect continous variable\n",
    "    conti_cols = [c[0] for c in df.dtypes if c[1] in ('int','bigint','double') and c[0] != 'churn_user_target']\n",
    "    # assemble continous variables\n",
    "    conti_assemb = VectorAssembler(inputCols = conti_cols, outputCol = \"conti_varas_vec\")\n",
    "    # scale continous variables\n",
    "    cat_scaler = StandardScaler(inputCol = \"conti_varas_vec\", outputCol = \"conti_scaled_feats\")\n",
    "    # update satage\n",
    "    stages += [conti_assemb, cat_scaler]\n",
    "    \n",
    "    \n",
    "    ### conver target variable into label indices\n",
    "    label_strIdx = StringIndexer(inputCol = \"churn_user_target\", outputCol = \"label\")\n",
    "    # update stage\n",
    "    stages += [label_strIdx]\n",
    "    \n",
    "    # Assemble vectors together\n",
    "    assembler_varas = [c + \"Index\" for c in cate_cols] + [\"conti_scaled_feats\"]\n",
    "    # Apply vectorAssembler\n",
    "    assembler = VectorAssembler(inputCols = assembler_varas, outputCol = \"features\")\n",
    "    # Update stage\n",
    "    stages += [assembler]\n",
    "    \n",
    "    ### ML model\n",
    "    ml_model  = model\n",
    "    stages += [ml_model]\n",
    "    \n",
    "    ### Compile pipeline\n",
    "    pipeline = Pipeline().setStages(stages)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4` Model Tranining and Predition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_contruct(df, model, paramGrid, evaluator):\n",
    "    \"\"\"Split train, test dataset. Grid Search best model by using TrainValidationSplit and do prediction on test set.   \n",
    "    INPUT:\n",
    "    df -- (dataframe): Dataframe with users as rows and features as columns.\n",
    "    model : machine learning alogrithm.\n",
    "    paramGrid: parameters for grid search.\n",
    "    evaluator : The evaluation method.\n",
    "    OUTPUT:\n",
    "    evaluator_val: model performance on validation dataset.\n",
    "    evaluator_test: model performance on test dataset.\n",
    "    \"\"\"\n",
    "    # build data process ml pipeline \n",
    "    pipeline = ml_pipeline(df, model)\n",
    "    \n",
    "    # split train test \n",
    "    train, test = df.randomSplit([0.8, 0.2], seed = 41)\n",
    "    \n",
    "    # split train-validation for finding the best param \n",
    "    train_val_model = TrainValidationSplit(estimator = pipeline,\n",
    "                               estimatorParamMaps = paramGrid,\n",
    "                               evaluator = evaluator,\n",
    "                               trainRatio = 0.7)\n",
    "    \n",
    "    # train model\n",
    "    ml_model = train_val_model.fit(train)\n",
    "    \n",
    "    # get each parameter setting's evaluation result list\n",
    "    evaluator_val = ml_model.validationMetrics\n",
    "    \n",
    "    # extract best model\n",
    "    ml_model_best = ml_model.bestModel\n",
    "    \n",
    "    # test result\n",
    "    result = ml_model_best.transform(test)\n",
    "    \n",
    "    # set evaluator\n",
    "    evaluator = evaluator\n",
    "    \n",
    "    # get evaluation result\n",
    "    evaluator_test = evaluator.evaluate(result)\n",
    "    \n",
    "    return evaluator_val, evaluator_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run all steps by using above functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and simple cleaning\n",
    "df_raw = data_prep(\"mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering on raw data\n",
    "df_fe = feature_engineer(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model contruct\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10, 15]) \\\n",
    "        .build()\n",
    "\n",
    "rf_val_result, rf_test_result = model_contruct(df_fe, rf, paramGrid, BinaryClassificationEvaluator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "**End to end solutions** <br>\n",
    "By far, we have explored digital music product user raw log data by using Spark. We've done a lot work like data cleaning, churn defining, data transformation, exploratary between churn/stay groups. At last, we built data processing and machine learning piplines. Using several algorithms and parameter tuning to train churn model and got fine predicted results on test set. \n",
    "\n",
    "**Difficutities** <br>\n",
    "The most difficutity in this project is not quite familiar with Spark library and function at beginning, especially after using a long time and get used to pandas and sklearn. But it is very good practice to handle a business project from end-to-end by using Spark.\n",
    "\n",
    "**Potential Improvements** <br>\n",
    "This project is pretty like the simple but complementary process about how to use Spark to process data and build machine learning models. However, lots of improvement can be done in the following steps, such as using the full size dataset and run on AWS clusters, try more alogorithms and tunning more paramenter etc. Continue to work and make this churn prediction model become scalable and fit the criteria into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
